{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **50.007 ML 1D Project**\n",
    "By Darren Chan Yu Hao"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import copy as copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "np.random.seed(1993)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read data\n",
    "\n",
    "# Read dev.in data\n",
    "def read_dev_in_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            results.append(line.strip())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read dev.out data\n",
    "def read_dev_out_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip().split(\" \")\n",
    "            results.append(stripped_line)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read train data\n",
    "def read_train_data(filepath):\n",
    "    results = []\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip().split(\" \")\n",
    "            results.append(stripped_line)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pdath of the data\n",
    "#------------------------------------\n",
    "# Spanish: ES\n",
    "ES_dev_in_data_path = os.path.join(\"Data\", \"ES\" , \"dev.in\")\n",
    "ES_dev_out_data_path = os.path.join(\"Data\", \"ES\" , \"dev.out\")\n",
    "ES_train_data_path = os.path.join(\"Data\", \"ES\" , \"train\")\n",
    "\n",
    "# Russiadn: RU\n",
    "RU_dev_in_data_path = os.path.join(\"Data\", \"RU\" , \"dev.in\")\n",
    "RU_dev_out_data_path = os.path.join(\"Data\", \"RU\" , \"dev.out\")\n",
    "RU_train_data_path = os.path.join(\"Data\", \"RU\" , \"train\")\n",
    "#------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words and tags\n",
    "def split_words_tags(labeled_data):\n",
    "        words = []\n",
    "        tags = []\n",
    "\n",
    "        for word_tag in labeled_data:\n",
    "            \n",
    "            if len(word_tag) != 2:\n",
    "                continue\n",
    "            \n",
    "            #word_tag is a list\n",
    "            word = word_tag[0]\n",
    "            tag = word_tag[1]\n",
    "\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "\n",
    "        return words, tags\n",
    "\n",
    "# Count unique tags\n",
    "def count_unique_tags(tags_ls):\n",
    "\n",
    "    tags_unique = set()\n",
    "    for tag in tags_ls:\n",
    "        tags_unique.add(tag)\n",
    "    return tags_unique\n",
    "\n",
    "# Count unique words\n",
    "def count_unique_words(words_ls):\n",
    "         \n",
    "    words_unique = set()\n",
    "    for word in words_ls:\n",
    "        words_unique.add(word)\n",
    "    return words_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emission Parameters\n",
    "\n",
    "# Get the emission parameters\n",
    "def get_emission_parameters(ls_of_tags, ls_of_words, tags, words, k=1):\n",
    "\n",
    "  # Write a function that estimates the emission parameters from the training set using MLE (maximumlikelihood estimation):\n",
    "    # e(x|y) = Count(y -> x) / Count(y)\n",
    "    # Count(y -> x) = Number of times word x is tagged with tag y\n",
    "    # Count(y) = Number of times tag y appears\n",
    "\n",
    "    # Input: ls_of_tags - list of unqiue tags\n",
    "    # Input: ls_of_words - list of unqiue words\n",
    "    # Input: tags - list of all tags\n",
    "    # Input: words - list of all words\n",
    "    # Output: emission_parameters\n",
    "\n",
    "    # emission_parameters is a dictionary where:\n",
    "        # The keys are (tag, word) tuples\n",
    "        # The values are the emission parameters e(x|y)\n",
    "\n",
    "    # Example of emission_parameters:\n",
    "        # emission_parameters[(\"O\", \"apple\")] = 0.00019841269\n",
    "        # emission_parameters[(\"B-positive\", \"apple\")] = 0.00000031622777\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    emission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (tag, word) tuple\n",
    "    count_y_to_x = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "    \n",
    "    print(f\"This is Count(y) : {count_y}\")\n",
    "\n",
    "    # Get the count of each (tag, word) tuple from the training set\n",
    "    for tag, word in zip(tags, words):\n",
    "        if (tag, word) in count_y_to_x:\n",
    "            count_y_to_x[(tag, word)] += 1\n",
    "        else:\n",
    "            count_y_to_x[(tag, word)] = 1\n",
    "\n",
    "    print(f\"This is Count(y -> x) : {count_y_to_x}\")\n",
    "\n",
    "    # Get the emission parameters\n",
    "    for tag, word in count_y_to_x:\n",
    "\n",
    "        emission_parameters[(tag, word)] = count_y_to_x[(tag, word)] / (count_y[tag] + k) # SOMETHING WRONG WITH THIS FORMULA\n",
    "        if word == \"con\":\n",
    "            print(tag,emission_parameters[(tag, word)])\n",
    "\n",
    "    # For words that do not appear in the training set, k/(Count(y)+k) is used as the emission parameter\n",
    "    unknown_word = \"UNK\"\n",
    "    for tag in count_y:\n",
    "        emission_parameters[(tag, unknown_word)] = k / (count_y[tag] + k)\n",
    "\n",
    "    print(f\"This is e(x|y) : {emission_parameters}\")\n",
    "\n",
    "    return emission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_estimate_tags(test_words, emission_params, train_ls_of_words):\n",
    "\n",
    "    # for each word in the test set of words (test_words) assign the tag with the highest emission probability\n",
    "\n",
    "    # Inputs : test_tags - a list of all tags\n",
    "    #          test_ls_of_tags - a list of unqiue tags\n",
    "    #         test_number_of_tags - a list of the number of tags\n",
    "    #        test_words - a list of all words\n",
    "    #       emission_params - a dictionary of emission parameters\n",
    "    # \n",
    "    # Output : labelled words - a list of words with their assigned tags\n",
    "\n",
    "    predicted_results = []\n",
    "\n",
    "    for word in test_words:\n",
    "        if word in train_ls_of_words:\n",
    "\n",
    "            # y∗ = arg max y e(x|y)\n",
    "            emission_value = 0\n",
    "            for key in emission_params:\n",
    "                if key[1] == word:\n",
    "                    if emission_value < emission_params[key]:\n",
    "                        emission_value = emission_params[key]\n",
    "                        value = key[0]\n",
    "            \n",
    "            predicted_results.append((word, value))\n",
    "            \n",
    "        else:\n",
    "\n",
    "            if word != \"\":\n",
    "                # y∗ = arg max y e(x|y)\n",
    "                emission_value = 0\n",
    "                for key in emission_params:\n",
    "                    if key[1] == \"UNK\":\n",
    "                        if emission_value < emission_params[key]:\n",
    "                            emission_value = emission_params[key]\n",
    "                            value = key[0]\n",
    "\n",
    "                predicted_results.append((\"UNK\", value))\n",
    "            \n",
    "            else:\n",
    "                predicted_results.append((\"\", \"\"))\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"predicted_results: \", predicted_results)\n",
    "    return predicted_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(test_labels, gold_standard):\n",
    "\n",
    "    total_predicted = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # convert to set for faster lookup\n",
    "    gold_standard_tuple_ver = []\n",
    "\n",
    "    for tuple in gold_standard:\n",
    "\n",
    "        if len(tuple) < 2:\n",
    "            continue\n",
    "\n",
    "        gold_standard_tuple_ver.append((tuple[0], tuple[1]))\n",
    "\n",
    "    print(f\"This is the gold standard: {gold_standard_tuple_ver} \\n\")\n",
    "\n",
    "    for predicted_pair in test_labels:\n",
    "\n",
    "        if predicted_pair in gold_standard_tuple_ver:\n",
    "            total_correct += 1\n",
    "    \n",
    "        total_predicted += 1\n",
    "\n",
    "    return total_correct / total_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(test_labels, gold_standard):\n",
    "\n",
    "    total_correct = 0\n",
    "    total_gold = 0\n",
    "\n",
    "    gold_standard_tuple_ver = []\n",
    "\n",
    "    for tuple in gold_standard:\n",
    "\n",
    "        if len(tuple) < 2:\n",
    "            continue\n",
    "\n",
    "        gold_standard_tuple_ver.append((tuple[0], tuple[1]))\n",
    "\n",
    "    for tuple in gold_standard_tuple_ver:\n",
    "\n",
    "        if tuple in test_labels:\n",
    "            total_correct += 1\n",
    "\n",
    "        total_gold += 1\n",
    "\n",
    "    return total_correct / total_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f_score(precision, recall):\n",
    "    return 2/((1/precision) + (1/recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_part_1(dev_in_data_path, dev_out_data_path, train_data_path, output_path):\n",
    "\n",
    "    train_data = read_train_data(train_data_path)\n",
    "\n",
    "    train_words, train_tags = split_words_tags(train_data)\n",
    "    train_ls_of_tags = count_unique_tags(train_tags)\n",
    "    train_ls_of_words = count_unique_words(train_words)\n",
    "\n",
    "    # Get Emmission Parameters\n",
    "    k = 1\n",
    "    emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, k)\n",
    "\n",
    "    test_data = read_dev_in_data(dev_in_data_path)\n",
    "\n",
    "    # Get labels for test data\n",
    "    test_labels = assign_estimate_tags(test_data, emission_params, train_ls_of_words)\n",
    "\n",
    "    with open(output_path, \"w+\", encoding=\"utf-8\") as file:\n",
    "        for line in test_labels:\n",
    "            write_line = line[0] + \" \" + line[1] + \"\\n\"\n",
    "            file.write(write_line)\n",
    "\n",
    "    gold_standard = read_dev_out_data(dev_out_data_path)\n",
    "\n",
    "    # Precision = Total number of correctly predicted entities / Total number of predicted entities\n",
    "\n",
    "    precision = get_precision(test_labels, gold_standard)\n",
    "    print(\"Precision: \", precision)\n",
    "\n",
    "    # Recall = Total number of correctly predicted entities / Total number of entities in the gold standard\n",
    "    recall = get_recall(test_labels, gold_standard)\n",
    "    print(\"Recall: \", recall)\n",
    "\n",
    "    # F score\n",
    "    f_score = get_f_score(precision, recall)\n",
    "    print(\"F Score: \", f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Language\n",
    "\n",
    "# For Spanish\n",
    "\n",
    "print(\"For Spanish: \")\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"dev.p1.out\")\n",
    "calculate_part_1(ES_dev_in_data_path, ES_dev_out_data_path, ES_train_data_path, output_path)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# For Russian\n",
    "print(\"For Russian: \")\n",
    "output_path = os.path.join(\"Data\", \"RU\" , \"dev.p1.out\")\n",
    "calculate_part_1(RU_dev_in_data_path, RU_dev_out_data_path, RU_train_data_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_train_data(ES_train_data_path)\n",
    "train_words, train_tags = split_words_tags(train_data)\n",
    "train_ls_of_tags = count_unique_tags(train_tags)\n",
    "train_ls_of_words = count_unique_words(train_words)\n",
    "# Get Emmission Parameters\n",
    "k = 1\n",
    "emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data_p2(filepath):\n",
    "    results = []\n",
    "    #Add start state\n",
    "    results.append(' START')\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            results.append(stripped_line)\n",
    "            if stripped_line == \"\":\n",
    "                # include stop and start states at new sentence\n",
    "                results.append(' STOP')\n",
    "                results.append(' START')\n",
    "    final_results = []\n",
    "    for line in results:\n",
    "        split_lines = line.split(\" \")\n",
    "        final_results.append(split_lines)\n",
    "    final_results.pop()\n",
    "    #remove final start state\n",
    "\n",
    "    return final_results\n",
    "\n",
    "print(read_train_data_p2(ES_train_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transmission_parameters(ls_of_tags, tags):\n",
    "    #Write a function that estimates the transition parameters from the training set using MLE (maximum likelihood estimation)\n",
    "    # q( y_i | y_i-1 ) = Count( y_i-1, y_i ) / Count( y_i-1 )\n",
    "    # Count(y_i-1 , y_i) = Number of times tag y_i-1 transits to tag y_i\n",
    "    # Count(y_i-1) = Number of times tag y_i-1 appears\n",
    "\n",
    "    # Input: ls_of_tags - list of unique tags\n",
    "    # Input: tags - list of all tags\n",
    "\n",
    "    # transmission_parameters is a dictionary where:\n",
    "        # The keys are (tag_y_i-1, tag_y_i) tuples\n",
    "        # The values are the transmission parameters q(y_i | y_i-1)\n",
    "\n",
    "    # Example of emission_parameters:\n",
    "        # emission_parameters[(\"O\", \"O\")] = 0.00019841269\n",
    "        # emission_parameters[(\"B-positive\", \"O\")] = 0.00000031622777\n",
    "\n",
    "    # Create a dictionary to store the emission parameters\n",
    "    transmission_parameters = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each tag\n",
    "    count_y = {}\n",
    "\n",
    "    # Create a dictionary to store the count of each (y_i-1, y_i) tuple\n",
    "    count_y_i_1_to_y_i = {}\n",
    "\n",
    "    # Get the count of each tag from the training set\n",
    "    for tag_labels in ls_of_tags:\n",
    "        count_y[tag_labels] = tags.count(tag_labels)\n",
    "\n",
    "    print(f\"This is Count(y) : {count_y}\")\n",
    "\n",
    "    # Get the count of each (y_i-1, y_i) tuple from the training set\n",
    "    for i in range(1, len(tags)):\n",
    "        if (tags[i-1], tags[i]) in count_y_i_1_to_y_i:\n",
    "            count_y_i_1_to_y_i[(tags[i-1],tags[i])] +=1\n",
    "        else:\n",
    "            count_y_i_1_to_y_i[(tags[i-1],tags[i])] =1\n",
    "\n",
    "    print(f\"This is Count (y_i-1 , y_i) : {count_y_i_1_to_y_i}\")\n",
    "\n",
    "    #transmission probability from state y_i-1 to y_i e.g (\"START\", \"O\") = 0.9281 == 0.9281 probability to transmit from \"START\" to \"O\" state\n",
    "\n",
    "    for key, value in count_y_i_1_to_y_i.items():\n",
    "        transmission_parameters[key] = value / count_y[key[0]]\n",
    "\n",
    "    print(f\"This is the q(y_i | y_i-1): {transmission_parameters}\")\n",
    "\n",
    "    labels = [\"START\", \"STOP\", \"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\"]\n",
    "    for i in labels:\n",
    "        for j in labels:\n",
    "            if (i, j) in transmission_parameters:\n",
    "                continue\n",
    "            else:\n",
    "                transmission_parameters[(i, j)] = 0\n",
    "\n",
    "    return transmission_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_train_data_p2(ES_train_data_path)\n",
    "train_words, train_tags = split_words_tags(train_data)\n",
    "train_ls_of_tags = count_unique_tags(train_tags)\n",
    "transmission_params = get_transmission_parameters(train_ls_of_tags,train_tags)\n",
    "print(transmission_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that if we just blindly shove the fractions into the algorithm\n",
    "# multiply fractions enough times and it'll approach 0\n",
    "# and yeah that's gonna end up becoming 0 ft. computer inaccuracy\n",
    "# that's the numerical underflow\n",
    "# we can prevent this by log-ing everything\n",
    "\n",
    "# both transition_parameters and emission_parameters are dictionaries\n",
    "\n",
    "def log_underflow_prevention(parameter_dict):\n",
    "    log_parameter_dict = {}\n",
    "    for key, value in parameter_dict.items():\n",
    "        if value == 0:\n",
    "            log_parameter_dict[key] = -np.inf\n",
    "        else:\n",
    "            log_parameter_dict[key] = np.log(value)\n",
    "    return log_parameter_dict\n",
    "\n",
    "print(log_underflow_prevention(transmission_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in the dev/test set (or any line-separated text file containing only words)\n",
    "def read_dev(path):\n",
    "  out = [[]]\n",
    "  f = open(path, \"r\", encoding=\"utf-8\")\n",
    "  lines_in = f.readlines()\n",
    "  for word in lines_in:\n",
    "    if word == \"\\n\":\n",
    "      out.append([])\n",
    "    else:\n",
    "      out[-1].append(word.rstrip())\n",
    "  return out[:-1]\n",
    "ES_dev_in_data = read_dev(ES_dev_in_data_path)\n",
    "print(ES_dev_in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(document, transmission, emission, ls_of_words):\n",
    "  n = len(document)\n",
    "  tags = [\"O\", \"B-positive\", \"B-neutral\", \"B-negative\", \"I-positive\",\"I-neutral\",\"I-negative\",\"STOP\"]\n",
    "\n",
    "  memo = [{} for _ in range(n+1)]\n",
    "  parent_arr = [{} for _ in range(n+1)]\n",
    "  #initial step from start to first node\n",
    "  for tag in tags:\n",
    "    a_v_u = transmission.get((\"START\", tag)) \n",
    "    if document[0] in ls_of_words:\n",
    "      # if tag emits word, get emission, else -inf\n",
    "      b_u = emission.get((tag, document[0])) or -np.inf\n",
    "    else:\n",
    "      #if word not in document\n",
    "      b_u = emission.get((tag, \"UNK\")) \n",
    "    memo[0][tag] =  a_v_u + b_u\n",
    "    parent_arr[0][tag] = None\n",
    "  #recursive\n",
    "  for j in range(1,n):\n",
    "    for u in tags:\n",
    "      max_prob = -np.inf\n",
    "      max_v = None\n",
    "      for v in tags:\n",
    "        if (memo[j][v] == -np.inf or transmission.get((v, u)) == -np.inf):\n",
    "          continue\n",
    "        if document[j] in ls_of_words:\n",
    "          emission_prob = emission.get((u, document[j])) or -np.inf\n",
    "        else:\n",
    "          emission_prob = emission.get((u, \"UNK\"))\n",
    "        prob = memo[j-1][v] + transmission.get((v, u)) + emission_prob\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            max_v = v\n",
    "      if max_prob == -np.inf:\n",
    "        continue\n",
    "      memo[j][u] = max_prob\n",
    "      parent_arr[j][u] = max_v\n",
    "  # Termination step\n",
    "\n",
    "  max_prob = -np.inf\n",
    "  max_v = None\n",
    "  for tag in tags:\n",
    "    if (memo[n-1][tag] == -np.inf or transmission.get((tag, \"STOP\"))== -np.inf):\n",
    "      continue\n",
    "    prob = memo[n-1][tag] + transmission.get((tag, \"STOP\"))\n",
    "    if prob > max_prob:\n",
    "        max_prob = prob\n",
    "        max_v = tag\n",
    "\n",
    "  if max_prob != -np.inf:\n",
    "    memo[n]['STOP'] = max_prob\n",
    "    parent_arr[n]['STOP'] = max_v\n",
    "    \n",
    "  most_likely_sequence = [\"\" for _ in range(n)]\n",
    "  if max_v == None:\n",
    "        max_v = \"O\"\n",
    "  # Backtrack to find the most likely path\n",
    "  for t in range(n , 0, -1):\n",
    "    print(t)\n",
    "    max_v = parent_arr[t].get(max_v)\n",
    "    if max_v == None:\n",
    "      max_v = \"O\"\n",
    "    most_likely_sequence[t-1] = max_v\n",
    "\n",
    "  return most_likely_sequence\n",
    "  \n",
    "  \n",
    "print(viterbi(ES_dev_in_data[0], log_underflow_prevention(transmission_params), log_underflow_prevention(emission_params), train_ls_of_words))\n",
    "print(viterbi(ES_dev_in_data[1], log_underflow_prevention(transmission_params), log_underflow_prevention(emission_params), train_ls_of_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_loop(data, transmission, emission, ls_of_words):\n",
    "  results =[]\n",
    "  for document in data:\n",
    "    results.append(viterbi(document, transmission, emission, ls_of_words))\n",
    "  return results\n",
    "\n",
    "results = viterbi_loop(ES_dev_in_data,log_underflow_prevention(transmission_params), log_underflow_prevention(emission_params), train_ls_of_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the prediction from trained data into the dev.in file and output\n",
    "def assign_prediction(prediction, data, path):\n",
    "    if (len(prediction) != len(data)):\n",
    "        return \"Error, prediction length != data length\"\n",
    "    file = open(path, \"w\", encoding=\"utf-8\")\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        assert( len(prediction[i])== len(data[i]))\n",
    "        m = len(data[i])\n",
    "        for j in range(m):\n",
    "            file.write(data[i][j] + \" \" + prediction[i][j] + \"\\n\")\n",
    "        file.write(\"\\n\")\n",
    "    print(\"Wrote predictions to\", path)\n",
    "    return\n",
    "\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"dev.p2.out\")\n",
    "\n",
    "print(assign_prediction(results, ES_dev_in_data, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_part_2(dev_in_data_path, dev_out_data_path, train_data_path, output_path):\n",
    "  #sort train data into tag and words\n",
    "  train_data = read_train_data(train_data_path)\n",
    "  train_words, train_tags = split_words_tags(train_data)\n",
    "  train_ls_of_tags = count_unique_tags(train_tags)\n",
    "  train_ls_of_words = count_unique_words(train_words)\n",
    "  # Get Emission Parameters\n",
    "  k = 1\n",
    "  emission_params = get_emission_parameters(train_ls_of_tags, train_ls_of_words, train_tags, train_words, k)\n",
    "\n",
    "  #append start and stop to train data per document\n",
    "  train_data_modified = read_train_data_p2(train_data_path)\n",
    "  train_words_modified, train_tags_modified = split_words_tags(train_data_modified)\n",
    "  train_ls_of_tags_modified = count_unique_tags(train_tags_modified)\n",
    "  transmission_params = get_transmission_parameters(train_ls_of_tags_modified, train_tags_modified)\n",
    "\n",
    "  #log transmission and emission params to avoid underflow\n",
    "  log_emission = log_underflow_prevention(emission_params)\n",
    "  log_transmission = log_underflow_prevention(transmission_params)\n",
    "  # read dev_in in list of list\n",
    "  dev_in_list = read_dev(dev_in_data_path)\n",
    "  \n",
    "  # run viterbi and get predictions for the whole dev_in\n",
    "  predictions = viterbi_loop(dev_in_list, log_transmission, log_emission, train_ls_of_words)\n",
    "  # write predictions into dev.p2.out\n",
    "  assign_prediction(predictions, dev_in_list, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Language\n",
    "\n",
    "# For Spanish\n",
    "\n",
    "print(\"For Spanish: \")\n",
    "output_path = os.path.join(\"Data\", \"ES\" , \"dev.p2.out\")\n",
    "calculate_part_2(ES_dev_in_data_path, ES_dev_out_data_path, ES_train_data_path, output_path)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# For Russian\n",
    "print(\"For Russian: \")\n",
    "output_path = os.path.join(\"Data\", \"RU\" , \"dev.p2.out\")\n",
    "calculate_part_2(RU_dev_in_data_path, RU_dev_out_data_path, RU_train_data_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "50.007ML-OdyoVhM6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
